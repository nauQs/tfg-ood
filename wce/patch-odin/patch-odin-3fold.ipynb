{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import imageio\n",
    "from skimage import transform,io\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import Counter \n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "if False: # only for JupyterLab with GPUs environment\n",
    "    import sys\n",
    "    sys.path.insert(0, '/notebooks/')\n",
    "    sys.path.insert(0, '../')\n",
    "    from CapsuleLib.utils import gpu_config, availableGPU\n",
    "    gpuid = gpu_config(False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-cement",
   "metadata": {},
   "source": [
    "## Paths setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL_IMAGES_PATH = \"/data/capri/polyp_db_v3/normal_dbv1\"  # set paths here!\n",
    "OOD_IMAGES_PATH = \"/notebooks/Arnau/patch-self-supervised/data/ood-images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-notice",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "PATCH_SIZE = (64,64)\n",
    "IM_SIZE = 256\n",
    "TEMP = 1000\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = NORMAL_IMAGES_PATH\n",
    "filepaths, videos = [], []\n",
    "for image in os.listdir(path):\n",
    "    if image.endswith('.png'):\n",
    "        image_id = image[:3]\n",
    "        filepaths.append(os.path.join(path,image))\n",
    "        videos.append(image_id)\n",
    "df = pd.DataFrame({\"path\": filepaths, \"video\":videos})\n",
    "df['video'] = df['video'].astype(int)\n",
    "\n",
    "folds = pd.read_csv('3fold.csv')\n",
    "\n",
    "path = OOD_IMAGES_PATH\n",
    "\n",
    "filepaths, image_ids = [], []\n",
    "for image in os.listdir(path):\n",
    "    if image.endswith('.png'):\n",
    "        image_id = image.split('-')[0][-3:]\n",
    "        filepaths.append(os.path.join(path,image))\n",
    "        image_ids.append(image_id)\n",
    "df_ood = pd.DataFrame({\"path\": filepaths, \"image\":image_ids})\n",
    "\n",
    "def rotate_image(image):\n",
    "    return np.rot90(image, np.random.choice([-1, 0, 1, 2]))\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255., vertical_flip=True,\n",
    "    horizontal_flip=True,preprocessing_function=rotate_image)\n",
    "\n",
    "ood_gen=datagen.flow_from_dataframe(dataframe=df_ood, x_col=\"path\", class_mode=None, \n",
    "    target_size=(IM_SIZE,IM_SIZE), batch_size=BATCH_SIZE, shuffle=True, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-functionality",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msp(image_batch):\n",
    "    \"\"\"\n",
    "    Returns the list of MSP given a batch of images\n",
    "    \"\"\"\n",
    "    softmax_probs = model.predict(image_batch)\n",
    "    return 1 - np.max(softmax_probs, axis=1)\n",
    "\n",
    "def sliding_window(image):\n",
    "    \"\"\"\n",
    "    Returns sixteen overlapping 64x64 patches from a 256x256 image\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for y in range(32, 192, 44):\n",
    "        for x in range(32, 192, 44):\n",
    "            windows.append(image[y:y+64, x:x+64])\n",
    "    return np.array(windows)\n",
    "\n",
    "def perturb_images(images, epsilon=0.002):\n",
    "    \"\"\"\n",
    "    Perturbs a batch of image for a given epsilon magnitude\n",
    "    \"\"\"\n",
    "    \n",
    "    test_ds_var = tf.Variable(images, trainable=True)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(test_ds_var)\n",
    "        logits = model(test_ds_var, training=False)\n",
    "        loss = -tf.reduce_mean(tf.reduce_max(logits, axis=1))\n",
    "\n",
    "    gradients = tape.gradient(loss, test_ds_var)\n",
    "    gradients = tf.math.greater_equal(gradients, 0)\n",
    "    gradients = tf.cast(gradients, tf.float32)\n",
    "    gradients = (gradients - 0.5) * 2\n",
    "\n",
    "    static_tensor = tf.convert_to_tensor(test_ds_var) - epsilon * gradients\n",
    "    static_tensor = tf.clip_by_value(static_tensor, 0., 255.)\n",
    "    \n",
    "    return static_tensor\n",
    "\n",
    "def get_hist_roc_patches(runs=100, epsilon=0, summary=\"max\"):\n",
    "    \"\"\"\n",
    "    Computes histogram and ROC for the given epsilon and summary function\n",
    "    \"\"\"\n",
    "    \n",
    "    msp_list = []\n",
    "    y_pred, y_true, y_probs = [], [], []\n",
    "    \n",
    "    for i, generator in enumerate([val_gen, ood_gen]):\n",
    "        msp = []\n",
    "        for n in range(runs):\n",
    "            batch = generator.next()\n",
    "            batch_msp = []\n",
    "            for image in batch:\n",
    "                patches = sliding_window(image)\n",
    "                k = len(patches)\n",
    "                if epsilon==0:\n",
    "                    msp_patches = get_msp(patches)\n",
    "                else:\n",
    "                    msp_patches = get_msp(perturb_images(patches, epsilon))\n",
    "                    \n",
    "                if summary==\"top5\": summary_patches = sum(sorted(msp_patches, reverse=True)[:5])/5.\n",
    "                if summary==\"max\": summary_patches = max(msp_patches)\n",
    "                if summary==\"avg\": summary_patches = sum(msp_patches)/k\n",
    "                    \n",
    "                assert 0 <= summary_patches <= 1\n",
    "                \n",
    "                batch_msp.append(summary_patches)\n",
    "                msp.append(summary_patches)\n",
    "                \n",
    "            if len(batch_msp) == BATCH_SIZE:\n",
    "                y_probs = np.concatenate([y_probs, batch_msp])\n",
    "                if i == 0:\n",
    "                    y_true = np.concatenate([y_true, np.zeros(BATCH_SIZE)])\n",
    "                else:\n",
    "                    y_true = np.concatenate([y_true, np.ones(BATCH_SIZE)])\n",
    "                    \n",
    "        msp_list.append(msp)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    logbins = np.logspace(-3,0,80)\n",
    "    ax.hist(msp_list[0], bins=logbins, alpha=0.4, label=\"Validation\", color=\"blue\")\n",
    "    ax.hist(msp_list[1], bins=logbins, alpha=0.5, label=\"OOD\", color=\"orange\")\n",
    "    plt.xlabel(\"1-MSP\")\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.title(\"MSP histograms\")\n",
    "    plt.savefig(f'results/{N}/hist{suffix}.png')\n",
    "\n",
    "    threshold_range = np.concatenate([[0.],np.logspace(-8,-4,50), np.logspace(-4,-1,100), np.logspace(-1,0,100)])\n",
    "    roc_values = []\n",
    "\n",
    "    for threshold in threshold_range:\n",
    "        y_pred = np.where(y_probs >= threshold, 1, 0)\n",
    "        sensitivity = np.sum(np.logical_and(y_pred,y_true))/np.sum(y_true)\n",
    "        specificity = np.sum(~np.logical_or(y_pred,y_true)/(y_pred.size-np.sum(y_true)))\n",
    "        roc_values.append((1-specificity, sensitivity))\n",
    "\n",
    "    revspec, sens = zip(*roc_values)\n",
    "    revspec, sens = np.array(revspec), np.array(sens)\n",
    "    spec = 1-np.array(revspec)\n",
    "\n",
    "    df_roc = pd.DataFrame({'spec':spec, 'sens':sens})\n",
    "    df_roc.to_csv(f'results/{N}/roc{suffix}.csv')\n",
    "        \n",
    "    return spec, sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "generators, models = [], []\n",
    "for fold in [1,2,3]:\n",
    "    \n",
    "    videos_val = folds[folds[f'fold-{fold}']]['video'].values\n",
    "    df_val = df[df.video.isin(videos_val)]\n",
    "    val_gen=datagen.flow_from_dataframe(dataframe=df_val, x_col=\"path\", class_mode=None,\n",
    "    target_size=(IM_SIZE,IM_SIZE), batch_size=BATCH_SIZE, shuffle=True, seed=1)\n",
    "    generators.append(val_gen)\n",
    "    \n",
    "    model_name = f'models/patches-classifier-{N}-t{TEMP}-f{fold}.hdf5'\n",
    "    model = keras.models.load_model(model_name)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-utility",
   "metadata": {},
   "source": [
    "## 3 fold ODIN evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in [10,15,20]:\n",
    "    \n",
    "    print(f\"\\n ************** N={N} ***************\")\n",
    "    \n",
    "    generators, models = [], []\n",
    "    for fold in [1,2]:\n",
    "    \n",
    "        videos_val = folds[folds[f'fold-{fold}']]['video'].values\n",
    "        df_val = df[df.video.isin(videos_val)]\n",
    "        val_gen=datagen.flow_from_dataframe(dataframe=df_val, x_col=\"path\", class_mode=None,\n",
    "        target_size=(IM_SIZE,IM_SIZE), batch_size=BATCH_SIZE, shuffle=True, seed=1)\n",
    "        generators.append(val_gen)\n",
    "\n",
    "        model_name = f'models/patches-classifier-{N}-t{TEMP}-f{fold}.hdf5'\n",
    "        model = keras.models.load_model(model_name)\n",
    "        models.append(model)\n",
    "    \n",
    "    output = \"\"\n",
    "    for summ in [ \"top5\", \"max\", \"avg\"]:\n",
    "        print(summ)\n",
    "        output += f\"{summ}\\n\"\n",
    "        for eps in [0, 0.0005,0.001]:\n",
    "\n",
    "            auroc_list = []\n",
    "            print(f\"  epsilon={eps}\")\n",
    "            output += f\"  epsilon={eps}\\n\"\n",
    "\n",
    "            for fold in [1,2]:\n",
    "\n",
    "                suffix = f'-{N}-{summ}-t{TEMP}-eps{eps}-f{fold}'\n",
    "                model = models[fold-1]\n",
    "                val_gen = generators[fold-1]\n",
    "\n",
    "                spec, sens = get_hist_roc_patches(epsilon=eps, summary=summ, runs=50)\n",
    "                auroc = round(np.abs(np.trapz(spec, np.array(sens))),5)\n",
    "                auroc_list.append(auroc)\n",
    "                print(f\"    fold {fold} AUROC: {auroc}\")\n",
    "                output += f\"    fold {fold} AUROC: {auroc}\\n\"\n",
    "\n",
    "            print(f\"  Mean AUROC: {np.mean(auroc_list)}\\n\")\n",
    "            output += f\"  Mean AUROC: {np.mean(auroc_list)}\\n\\n\"\n",
    "\n",
    "    with open(f'results/output-{N}.txt', 'w') as f:\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-denver",
   "metadata": {},
   "source": [
    "## Plot clean histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "FOLD = 2\n",
    "model_name = f'models/patches-classifier-{K}-t{TEMP}-f{FOLD}.hdf5'\n",
    "model = keras.models.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_methods(epsilon, runs=50):\n",
    "    \n",
    "    for summary in [\"max\", \"top5\", \"avg\"]:\n",
    "        \n",
    "        print(summary)\n",
    "        \n",
    "        msp_list = []\n",
    "        for i, generator in enumerate([val_gen, ood_gen]):\n",
    "            msp = []\n",
    "            for n in range(runs):\n",
    "                batch = generator.next()\n",
    "                batch_msp = []\n",
    "                for image in batch:\n",
    "                    patches = sliding_window(image)\n",
    "                    k = len(patches)\n",
    "                    if epsilon==0:\n",
    "                        msp_patches = get_msp(patches)\n",
    "                    else:\n",
    "                        msp_patches = get_msp(perturb_images(patches, epsilon))\n",
    "\n",
    "                    if summary==\"top5\": summary_patches = sum(sorted(msp_patches, reverse=True)[:5])/5.\n",
    "                    if summary==\"max\": summary_patches = max(msp_patches)\n",
    "                    if summary==\"avg\": summary_patches = sum(msp_patches)/k \n",
    "\n",
    "                    batch_msp.append(summary_patches)\n",
    "                    msp.append(summary_patches)\n",
    "            msp_list.append(msp)\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(4,3), dpi=100)\n",
    "        logbins = np.logspace(-1,0,50)\n",
    "        ax.hist(msp_list[0], bins=logbins, alpha=0.4, label=\"Validation\", color=\"blue\")\n",
    "        ax.hist(msp_list[1], bins=logbins, alpha=0.5, label=\"OOD\", color=\"orange\")\n",
    "        ax.set_xlabel(\"1-$\\mathcal{S}(x)$\")\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xscale('log')\n",
    "        ax.legend()\n",
    "        plt.savefig(f'results/tfg-hist-{summary}.png', bbox_inches='tight')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_methods(0.0001,runs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-wrapping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupyter_starters": {
   "starter": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
